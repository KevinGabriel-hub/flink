apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-client-config-ha
  labels:
    app: flink
  namespace: flink
data:
  flink-conf.yaml: |+
    jobmanager.rpc.address: flink-jobmanager-ha
    taskmanager.numberOfTaskSlots: 3
    rest.address: flink-jobmanager-ha
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
    queryable-state.proxy.ports: 6125
    jobmanager.memory.process.size: 1600m
    taskmanager.memory.process.size: 1728m
    parallelism.default: 2  

    # Flink HA configurations
    kubernetes.cluster-id: flink-ha
    high-availability.type: kubernetes
    high-availability.storageDir: s3a://flink-ha/recovery
    restart-strategy.type: fixed-delay
    restart-strategy.fixed-delay.attempts: 10
    kubernetes.namespace: flink
    # state.backend.type: filesystem
    state.backend.type: rocksdb
    state.backend.rocksdb.localdir: /opt/flink/rocksdb-state
    state.backend.fs.hdfs.hadoopconf: s3a://minio-service:9000/
    state.backend.incremental: true
    state.checkpoints.dir: s3a://flink-state/flink/checkpoints
    state.savepoints.dir: s3a://flink-state/flink/savepoints
    fs.s3a.access.key: minio
    fs.s3a.secret.key: minio123 
    fs.s3a.endpoint: http://minio-service:9000
    fs.s3a.connection.timeout: 2000
    fs.s3a.connection.establish.timeout: 2000
    fs.s3a.connection.ssl.enabled: false
    execution.checkpointing.interval: 5000
    s3.path.style.access: true
    containerized.master.env.ENABLE_BUILT_IN_PLUGINS: "flink-s3-fs-hadoop-1.18.0.jar"
    containerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS: "flink-s3-fs-hadoop-1.18.0.jar"
    rest.flamegraph.enabled: true
    metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
  log4j-cli.properties: |+
    # This affects logging for both user code and Flink
    rootLogger.level = WARN
    rootLogger.appenderRef.console.ref = ConsoleAppender
    rootLogger.appenderRef.rolling.ref = RollingFileAppender

    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO

    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO

    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Log all infos in the given rolling file
    appender.rolling.name = RollingFileAppender
    appender.rolling.type = RollingFile
    appender.rolling.append = false
    appender.rolling.fileName = ${sys:log.file}
    appender.rolling.filePattern = ${sys:log.file}.%i
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 10

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF    
  demo-kafka-consumer.sql: |+
    CREATE TABLE alerts (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );

    CREATE TABLE alerts_windows (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts-windows',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );

    CREATE TABLE alerts_linux (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts-linux',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );
  job4.py: |+
    from pyflink.table.table_environment import StreamTableEnvironment
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.common import Types, WatermarkStrategy
    from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
    from pyflink.datastream.formats.json import JsonRowDeserializationSchema
    from pyflink.datastream.serialization import SimpleDeserializationSchema
    import json

    # Create a StreamExecutionEnvironment
    env = StreamExecutionEnvironment.get_execution_environment()
    # Create a TableEnvironment
    t_env = StreamTableEnvironment.create(env)


    # Define the schema for Kafka
    row_type_info = Types.ROW_NAMED(['user_id', 'item_id', 'category_id', 'behavior', 'ts', 'original_json'],
                                    [Types.STRING(), Types.STRING(), Types.STRING(), Types.STRING(), Types.STRING(), Types.STRING()])
    #json_deserialization_schema = JsonRowDeserializationSchema.builder().type_info(row_type_info).ignore_parse_errors().build()

    class CustomJsonDeserializationSchema(SimpleDeserializationSchema):
        def __init__(self, row_type_info):
            self.row_type_info = row_type_info

        def deserialize(self, message):
            try:
                json_object = json.loads(message)
                user_id = json_object.get('user_id', None)
                item_id = json_object.get('item_id', None)
                category_id = json_object.get('category_id', None)
                behavior = json_object.get('behavior', None)
                ts = json_object.get('ts', None)

                return user_id, item_id, category_id, behavior, ts
            except json.JSONDecodeError:
                # Return None for malformed JSON
                return None

    # Create a custom deserialization schema
    custom_json_deserialization_schema = CustomJsonDeserializationSchema(row_type_info)

    # Define the KafkaSource using the builder syntax
    source = KafkaSource.builder() \
        .set_bootstrap_servers("kafka-kafka-bootstrap:9095") \
        .set_topics("source") \
        .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \
        .set_value_only_deserializer(custom_json_deserialization_schema) \
        .build()

    ds = env.from_source(source, WatermarkStrategy.no_watermarks(), "Source")

    # interpret the insert-only DataStream as a Table
    source_table = t_env.from_data_stream(ds, custom_json_deserialization_schema)

    t_env.create_temporary_view("source", source_table)

    create_table_valid_records = """
        CREATE TABLE sink (
            user_id STRING,
            item_id STRING,
            category_id STRING,
            behavior STRING,
            ts STRING
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'sink',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9095',
            'format' = 'json'
        );
    """

    create_table_error_records = """
        CREATE TABLE sink_error (
            original_json STRING
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'sink_error',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9095',
            'format' = 'json'
        );
    """

    # Execute CREATE TABLE statements
    t_env.execute_sql(create_table_valid_records)
    t_env.execute_sql(create_table_error_records)

    stmt_set = t_env.create_statement_set()

    stmt_set \
        .add_insert_sql("INSERT INTO sink SELECT user_id, item_id, category_id, behavior, ts FROM source")
    stmt_set \
        .add_insert_sql("INSERT INTO sink_error SELECT original_json FROM source")

    stmt_set.execute()