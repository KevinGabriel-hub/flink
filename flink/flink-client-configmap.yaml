apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-client-config
  labels:
    app: flink
  namespace: flink
data:
  flink-conf.yaml: |+
    jobmanager.rpc.address: flink-jobmanager
    taskmanager.numberOfTaskSlots: 3
    rest.address: flink-jobmanager
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
    queryable-state.proxy.ports: 6125
    jobmanager.memory.process.size: 1600m
    taskmanager.memory.process.size: 1728m
    parallelism.default: 2  
    # state.backend.type: filesystem
    state.backend.type: rocksdb
    state.backend.rocksdb.localdir: /opt/flink/rocksdb-state
    state.backend.fs.hdfs.hadoopconf: s3a://minio-service:9000/
    state.backend.incremental: true
    state.checkpoints.dir: s3a://flink-state/flink/checkpoints
    state.savepoints.dir: s3a://flink-state/flink/savepoints
    fs.s3a.access.key: minio
    fs.s3a.secret.key: minio123 
    fs.s3a.endpoint: http://minio-service:9000
    fs.s3a.connection.timeout: 500
    fs.s3a.connection.establish.timeout: 500
    fs.s3a.connection.ssl.enabled: false
    execution.checkpointing.interval: 100
    s3.path.style.access: true
    containerized.master.env.ENABLE_BUILT_IN_PLUGINS: "flink-s3-fs-hadoop-1.18.0.jar"
    containerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS: "flink-s3-fs-hadoop-1.18.0.jar"
    rest.flamegraph.enabled: true
    metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
  log4j-cli.properties: |+
    # This affects logging for both user code and Flink
    rootLogger.level = WARN
    rootLogger.appenderRef.console.ref = ConsoleAppender
    rootLogger.appenderRef.rolling.ref = RollingFileAppender

    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO

    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO

    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Log all infos in the given rolling file
    appender.rolling.name = RollingFileAppender
    appender.rolling.type = RollingFile
    appender.rolling.append = false
    appender.rolling.fileName = ${sys:log.file}
    appender.rolling.filePattern = ${sys:log.file}.%i
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 10

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF    
  demo-kafka-consumer.sql: |+
    CREATE TABLE alerts (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );

    CREATE TABLE alerts_windows (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts-windows',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );

    CREATE TABLE alerts_linux (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts-linux',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );
  job1.py: |+
    from pyflink.table import EnvironmentSettings, TableEnvironment

    table_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
    table_env.get_config().set("parallelism.default", "1")

    # Define your CREATE TABLE statements
    create_table_alerts = """
        CREATE TABLE alerts (
          alert_id INT,
          host STRING,
          status STRING,
          os_type STRING,
          ts TIMESTAMP(3)
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'alerts',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
            'format' = 'json'
        );
    """

    create_table_windows_sink = """
        CREATE TABLE alerts_windows (
          alert_id INT,
          host STRING,
          status STRING,
          os_type STRING,
          ts TIMESTAMP(3)
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'alerts-windows',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
            'format' = 'json'
        );
    """

    # Execute CREATE TABLE statements
    table_env.execute_sql(create_table_alerts)
    table_env.execute_sql(create_table_windows_sink)

    # Define your INSERT INTO query
    insert_query = """
        INSERT INTO alerts_windows
        SELECT alert_id, host, status, os_type, ts FROM alerts WHERE os_type = 'windows'
    """

    # Execute the INSERT INTO query
    table_env.execute_sql(insert_query).wait()
  job2.py: |+
    from pyflink.common import Types, WatermarkStrategy
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.datastream.connectors.kafka import KafkaSource, KafkaSink, KafkaOffsetsInitializer, KafkaRecordSerializationSchema
    from pyflink.datastream.formats.json import JsonRowDeserializationSchema, JsonRowSerializationSchema
    from pyflink.table.table_environment import StreamTableEnvironment

    # Create a StreamExecutionEnvironment
    env = StreamExecutionEnvironment.get_execution_environment()
    # Create a TableEnvironment
    t_env = StreamTableEnvironment.create(env)


    # Define the deserialization schema for Kafka consumer
    row_type_info = Types.ROW_NAMED(['alert_id', 'host', 'status', 'os_type', 'ts'],
                                    [Types.INT(), Types.STRING(), Types.STRING(), Types.STRING(), Types.STRING()])
    json_deserialization_schema = JsonRowDeserializationSchema.builder().type_info(row_type_info).build()

    # Define the KafkaSource using the builder syntax
    source = KafkaSource.builder() \
        .set_bootstrap_servers("kafka-kafka-bootstrap:9092") \
        .set_topics("alerts") \
        .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \
        .set_value_only_deserializer(json_deserialization_schema) \
        .build()  

    # Add KafkaSource as a source to the environment
    ds = env.from_source(source, WatermarkStrategy.no_watermarks(), "Kafka Source")


    # Filter records where "behavior" is equal to "pv1"
    filtered_ds = ds.filter(lambda value: value['os_type'] == 'linux')

    # Register the DataStream as a temporary table
    #t_env.create_temporary_view("my_table", ds)
    # Use SQL-like expressions to filter records
    #filtered_table = t_env.sql_query("SELECT * FROM my_table WHERE os_type = 'linux'")
    # Convert the result back to a DataStream
    #filtered_ds = t_env.to_data_stream(filtered_table)


    # Define the serialization schema for Kafka producer
    json_serialization_schema = JsonRowSerializationSchema.builder().with_type_info(row_type_info).build()

    sink = KafkaSink.builder() \
        .set_bootstrap_servers("kafka-kafka-bootstrap:9092") \
        .set_record_serializer(
            KafkaRecordSerializationSchema.builder()
                .set_topic("alerts-linux")
                .set_value_serialization_schema(json_serialization_schema)
                .build()
        ) \
        .build()

    # Add KafkaSink as a sink to the environment
    filtered_ds.sink_to(sink)


    # Execute the job
    env.execute()
  job3.py: |+
    from pyflink.table import EnvironmentSettings, TableEnvironment
    from pyflink.table.expressions import col, call

    table_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
    table_env.get_config().set("parallelism.default", "1")

    create_table_linux_sink = """
        CREATE TABLE alerts_linux (
          alert_id INT,
          host STRING,
          status STRING,
          os_type STRING,
          ts STRING
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'alerts-linux',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
            'format' = 'json'
        );
    """
    table_env.execute_sql(create_table_linux_sink)

    # Create the table from a sql query
    table = table_env.sql_query("SELECT * FROM alerts_linux")

    result_table = table \
        .group_by(col("alert_id"), col("os_type")) \
        .select(col("alert_id"), col("os_type"), call("count", col("alert_id")).alias("_count_"))


    result_table.execute().print()
