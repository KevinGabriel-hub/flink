apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-client-config
  labels:
    app: flink
  namespace: flink
data:
  flink-conf.yaml: |+
    jobmanager.rpc.address: flink-jobmanager
    taskmanager.numberOfTaskSlots: 3
    rest.address: flink-jobmanager
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
    queryable-state.proxy.ports: 6125
    jobmanager.memory.process.size: 1600m
    taskmanager.memory.process.size: 1728m
    parallelism.default: 2  
    # state.backend.type: filesystem
    state.backend.type: rocksdb
    state.backend.rocksdb.localdir: /opt/flink/rocksdb-state
    state.backend.fs.hdfs.hadoopconf: s3a://minio-service:9000/
    state.backend.incremental: true
    state.checkpoints.dir: s3a://flink-state/flink/checkpoints
    state.savepoints.dir: s3a://flink-state/flink/savepoints
    fs.s3a.access.key: minio
    fs.s3a.secret.key: minio123 
    fs.s3a.endpoint: http://minio-service:9000
    fs.s3a.connection.timeout: 500
    fs.s3a.connection.establish.timeout: 500
    fs.s3a.connection.ssl.enabled: false
    execution.checkpointing.interval: 100
    s3.path.style.access: true
    containerized.master.env.ENABLE_BUILT_IN_PLUGINS: "flink-s3-fs-hadoop-1.18.0.jar"
    containerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS: "flink-s3-fs-hadoop-1.18.0.jar"
    rest.flamegraph.enabled: true
    metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
  log4j-cli.properties: |+
    # This affects logging for both user code and Flink
    rootLogger.level = WARN
    rootLogger.appenderRef.console.ref = ConsoleAppender
    rootLogger.appenderRef.rolling.ref = RollingFileAppender

    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO

    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO

    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Log all infos in the given rolling file
    appender.rolling.name = RollingFileAppender
    appender.rolling.type = RollingFile
    appender.rolling.append = false
    appender.rolling.fileName = ${sys:log.file}
    appender.rolling.filePattern = ${sys:log.file}.%i
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 10

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF    
  demo-kafka-consumer.sql: |+
    CREATE TABLE alerts (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );

    CREATE TABLE alerts_windows (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts-windows',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );

    CREATE TABLE alerts_linux (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts-linux',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );
  job1.py: |+
    from pyflink.table import EnvironmentSettings, TableEnvironment

    table_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
    table_env.get_config().set("parallelism.default", "1")

    # Define your CREATE TABLE statements
    create_table_users_gen = """
        CREATE TABLE users_gen (
            user_id INT,
            user_value DECIMAL(8,2),
            user_name STRING,
            user_time TIMESTAMP(3),
            user_special BOOLEAN
        ) WITH (
            'connector' = 'datagen',
            'fields.user_id.kind' = 'sequence',
            'fields.user_id.start' = '1',
            'fields.user_id.end' = '1000',
            'number-of-rows' = '1000',
            'fields.user_name.length' = '10',
            'rows-per-second' = '10',
            'fields.user_value.min' = '10.0',
            'fields.user_value.max' = '100.0',
            'fields.user_name.kind' = 'random',
            'fields.user_special.kind' = 'random'
        );
    """

    create_table_normal_users_sink = """
        CREATE TABLE users (
            user_id INT,
            user_value DECIMAL(8,2),
            user_name STRING,
            user_time TIMESTAMP(3),
            user_special BOOLEAN
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'users',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
            'format' = 'json'
        );
    """

    # Execute CREATE TABLE statements
    table_env.execute_sql(create_table_users_gen)
    table_env.execute_sql(create_table_normal_users_sink)

    # Define INSERT INTO query
    insert_query = """
      INSERT INTO users
      SELECT user_id, user_value, user_name, user_time, user_special FROM users_gen WHERE user_special = false
    """

    # Execute the INSERT INTO query
    table_env.execute_sql(insert_query).wait()
  job2.py: |+
    from pyflink.common import Types
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.datastream.connectors.kafka import KafkaSink, KafkaRecordSerializationSchema
    from pyflink.datastream.formats.json import JsonRowSerializationSchema
    from pyflink.table.table_environment import StreamTableEnvironment

    # Create a StreamExecutionEnvironment
    env = StreamExecutionEnvironment.get_execution_environment()
    # Create a TableEnvironment
    t_env = StreamTableEnvironment.create(env)


    t_env.execute_sql("""
        CREATE TABLE messages_gen (
            message_id INT,
            message_body STRING,
            message_time STRING,
            message_level INT
        ) WITH (
            'connector' = 'datagen',
            'fields.message_id.kind' = 'sequence',
            'fields.message_id.start' = '1',
            'fields.message_id.end' = '1000',
            'number-of-rows' = '1000',
            'fields.message_body.length' = '50',
            'fields.message_body.kind' = 'random',
            'rows-per-second' = '10',
            'fields.message_level.min' = '1',
            'fields.message_level.max' = '2'
        );
    """)

    # Conversion between Table and DataStream
    ds = t_env.to_append_stream(
        t_env.from_path('messages_gen'),
            Types.ROW_NAMED(['message_id', 'message_body', 'message_time', 'message_level'],
                                [Types.INT(), Types.STRING(), Types.STRING(), Types.INT()])
    )



    # Filter records
    #filtered_ds = ds.filter(lambda value: value['message_level'] == 1)
    filtered_ds = ds.filter(lambda value: print(value) or int(value['message_level']) == 1)



    # Define the schema for Kafka
    row_type_info = Types.ROW_NAMED(['message_id', 'message_body', 'message_time', 'message_level'],
                                    [Types.INT(), Types.STRING(), Types.STRING(), Types.INT()])
    json_serialization_schema = JsonRowSerializationSchema.builder().with_type_info(row_type_info).build()

    sink = KafkaSink.builder() \
        .set_bootstrap_servers("kafka-kafka-bootstrap:9092") \
        .set_record_serializer(
            KafkaRecordSerializationSchema.builder()
                .set_topic("messages")
                .set_value_serialization_schema(json_serialization_schema)
                .build()
        ) \
        .build()

    # Add KafkaSink as a sink to the environment
    filtered_ds.sink_to(sink)


    # Execute the job
    env.execute()
  job3.py: |+
    from pyflink.table import EnvironmentSettings, TableEnvironment

    table_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
    table_env.get_config().set("parallelism.default", "1")

    # Define CREATE TABLE statements
    create_table_users = """
        CREATE TABLE users (
            user_id INT,
            user_value DECIMAL(8,2),
            user_name STRING,
            user_time TIMESTAMP(3),
            user_special BOOLEAN
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'users',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
            'format' = 'json'
        );
    """

    create_table_messages = """
        CREATE TABLE messages (
            message_id INT,
            message_body STRING,
            message_time STRING,
            message_level INT
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'messages',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
            'format' = 'json'
        );
    """

    create_table_results_sink = """
        CREATE TABLE results (
            user_id INT,
            user_value DECIMAL(8,2),
            message_id INT,
            message_body STRING
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'results',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
            'format' = 'json'
        );
    """

    # Execute CREATE TABLE statements
    table_env.execute_sql(create_table_users)
    table_env.execute_sql(create_table_messages)
    table_env.execute_sql(create_table_results_sink)

    # Define INSERT INTO query
    insert_query = """
        INSERT INTO results
        SELECT u.user_id, u.user_value, m.message_id, m.message_body
            FROM users AS u
              JOIN messages AS m
                ON u.user_id = m.message_id;
    """

    # Execute the INSERT INTO query
    table_env.execute_sql(insert_query).wait()
